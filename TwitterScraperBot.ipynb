{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62fd869",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "\n",
    "## Might need to install modules below\n",
    "#!pip install selenium\n",
    "#!pip install textblob\n",
    "#!pip install dataframe_image\n",
    "#!pip install wordcloud #or in anaconda prompt type: conda install -c https://conda.anaconda.org/conda-forge wordcloud\n",
    "\n",
    "import os    \n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dataframe_image as dfi\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from time import sleep\n",
    "from textblob import TextBlob\n",
    "from selenium import webdriver\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66db6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### FUNCTIONS ###\n",
    "#################\n",
    "\n",
    "###########################\n",
    "# Scrape data from tweets #\n",
    "###########################\n",
    "\n",
    "def scrape_tweet(tweet, term):\n",
    "    \n",
    "    '''Extract the data from tweets related to the searched term'''\n",
    "    \n",
    "    #find username\n",
    "    username = tweet.find_element_by_xpath('.//span').text\n",
    "    \n",
    "    #find twitter handle\n",
    "    try:\n",
    "        handle = tweet.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException as error:\n",
    "        handle = None\n",
    "    \n",
    "    #find datetime of tweet\n",
    "    try:\n",
    "        postdate = tweet.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException as error:\n",
    "        postdate = None\n",
    "        \n",
    "    #get the post's text\n",
    "    try:\n",
    "        text = tweet.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    except NoSuchElementException as error:\n",
    "        text = None\n",
    "\n",
    "    #get associated #'s\n",
    "    associated_hashtags = []\n",
    "    for word in text.split():\n",
    "        if '#' in word and word.upper() != term.upper():\n",
    "            associated_hashtags.append(word)\n",
    "    \n",
    "    #replies count\n",
    "    replies = tweet.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    if replies == '':\n",
    "        replies = 0\n",
    "    \n",
    "    #retweets count\n",
    "    retweets = tweet.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    if retweets == '':\n",
    "        retweets = 0\n",
    "        \n",
    "    #likes count\n",
    "    likes = tweet.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    if likes == '':\n",
    "        likes = 0\n",
    "    \n",
    "    #making tuple of the extracted tweet data\n",
    "    data = (username, handle, postdate, text, associated_hashtags, replies, retweets, likes)\n",
    "    return data\n",
    "\n",
    "#######################################################\n",
    "# Ask user to start scraping when prompted with 'yes' #\n",
    "#######################################################\n",
    "\n",
    "def initiate_scraping(term):\n",
    "    no = 'no'\n",
    "    yes = 'yes'\n",
    "\n",
    "    initiate = input('Ready to start scraping on' + str(term) + '? Please enter \"yes\" or \"no\": ')\n",
    "\n",
    "    while initiate.upper() != yes.upper():\n",
    "        if initiate.upper() == no.upper():\n",
    "            initiate = input('No worries, take your time. Please enter \"yes\" when you are ready: ')\n",
    "        else:\n",
    "            initiate = input('Not an available option. Please enter \"yes\" or \"no\": ')\n",
    "    print(\"OK, it's scraping time!\")\n",
    "\n",
    "#####################################################################\n",
    "# Confirm the number of data is equal to max_data (1000 by default) #\n",
    "#####################################################################\n",
    "\n",
    "def max_data_reached(data):\n",
    "    if (len(data) == max_data):\n",
    "        print('Latest 1000 tweets on ' + str(term) + ' has been successfully scraped!\\n\\nMoving on to the data analysis...' )    \n",
    "    #the program can still carry on if the number of scraped data is more or less than max_data, it would simply be unexpected\n",
    "    else:\n",
    "        print('There were actually ' + str(len(data)) + ' tweets that got scraped instead of ' + str(max_data) + \n",
    "              '.\\nYou may want to restart the bot if that is not what you wanted. Moving on to the analysis...')\n",
    "\n",
    "#################################\n",
    "# Save the data into a csv file #\n",
    "#################################\n",
    "\n",
    "def make_csv(data):\n",
    "    with open('scraped_tweets.csv', 'w', newline = '', encoding = 'utf-8') as f:\n",
    "        header = ['Username', 'Handle', 'Timestamp', 'Text', 'Associated Hashtags', 'Replies', 'Retweets', 'Likes']\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "        print('Data are now stored in an csv file named \"scraped_tweets.csv\".')\n",
    "\n",
    "########################\n",
    "# Analysis of the data #\n",
    "########################\n",
    "\n",
    "##Sentiment analysis of the tweets' text using Textblob\n",
    "def sentiment_analysis(tweet):\n",
    "    def get_Subjectivity(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "    #Create a function to get the polarity\n",
    "    def get_Polarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    #Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "    tweet['TextBlob_Subjectivity'] = tweet['Text'].apply(get_Subjectivity)\n",
    "    tweet ['TextBlob_Polarity'] = tweet['Text'].apply(get_Polarity)\n",
    "    \n",
    "    #Create a column with string values of the sentiment analysis\n",
    "    def get_Analysis(score):\n",
    "        if score < 0:\n",
    "            return 'Negative'\n",
    "        elif score == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "    tweet['TextBlob_Analysis'] = tweet['TextBlob_Polarity'].apply(get_Analysis)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "##Generate word clouds (will be applied to text and associated hashtags columns of df)\n",
    "def make_WordCloud(column):\n",
    "    comment_words = ''\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # iterate through the csv file\n",
    "    for val in column:\n",
    "\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "\n",
    "        # converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "    if pd.DataFrame(column).columns[0] == 'Text':\n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color = 'white',\n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "    else:\n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color = 'black',\n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "        \n",
    "    # plot the word cloud image                      \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    if pd.DataFrame(column).columns[0] == 'Text':\n",
    "        plt.title(\"Word Cloud of the Tweets' text section\")\n",
    "    else:\n",
    "        plt.title(\"Word Cloud of Tweets' Associated Hashtags(#)\")   \n",
    "    \n",
    "    #save each word cloud as a png to tweet out\n",
    "    if pd.DataFrame(column).columns[0] == 'Text':\n",
    "        plt.savefig('wordcloud_text.png', format = 'png')\n",
    "    else:\n",
    "        plt.savefig('wordcloud_hashtags.png', format = 'png')\n",
    "    \n",
    "    plt.show()  \n",
    "\n",
    "##Function to make pie chart of the sentiment analysis\n",
    "def make_pieChart(df, term):    \n",
    "    #retrieve number of positive, neutral and negative tweets\n",
    "    pos = len(df['TextBlob_Analysis'][df['TextBlob_Analysis'] == 'Positive'])\n",
    "    neu = len(df['TextBlob_Analysis'][df['TextBlob_Analysis'] == 'Neutral'])\n",
    "    neg = len(df['TextBlob_Analysis'][df['TextBlob_Analysis'] == 'Negative'])\n",
    "    sentiments = [pos, neu, neg]\n",
    "    labels = 'Positive (' + str(pos) + ')', 'Neutral (' + str(neu) + ')', 'Negative (' + str(neg) + ')'\n",
    "\n",
    "    #setting up the pie chart plot\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.pie(sentiments, labels = labels, colors = ['cyan','yellow','violet'], autopct = '%1.1f%%',\n",
    "            shadow = False, startangle = 90)\n",
    "    ax.axis('equal')  #equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title('Pie Chart of the Sentiment Analysis on the latest 1000 tweets for ' + str(term))\n",
    "    plt.legend(title = 'Sentiment:', loc = 'best')\n",
    "\n",
    "    #saving pie chart to be tweeted out\n",
    "    plt.savefig('piechart_sentiment.png', format = 'png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "##Generate a table showing the total count of replies, retweets and likes grouped by sentiment (named as the variable RRL_table)\n",
    "def make_RRL_table(df):\n",
    "    #converting 'Likes' column values from str to int in cases when Likes have K & M as thousands and millions\n",
    "    if type(df['Likes'][0]) == str:\n",
    "        df['Likes'] = df['Likes'].replace({'K': '*1e3', 'M': '*1e6'}, regex=True).map(pd.eval).astype(int)\n",
    "\n",
    "    #making the RRL_table\n",
    "    RRL_table = df.groupby(by = 'TextBlob_Analysis').sum().iloc[:,0:3]\n",
    "    RRL_table = RRL_table.rename(columns = {'Replies':'Total Replies',\n",
    "                                            'Retweets':'Total Retweets', 'Likes':'Total Likes'})\n",
    "    RRL_table.index.name = None\n",
    "    #store the table as an image to be tweeted out\n",
    "    RRL_table_styled = RRL_table.style.background_gradient() #add gradient base on the value of the cells\n",
    "    #saving table as png\n",
    "    dfi.export(RRL_table_styled,'RRL_table.png')\n",
    "    #viewing the table\n",
    "    print(RRL_table)\n",
    "\n",
    "####################\n",
    "# Show the results #\n",
    "####################\n",
    "\n",
    "def show_results():\n",
    "\n",
    "    ##Making the scraped_tweets.csv file a dataframe \n",
    "    df = pd.read_csv('../TwitterBot/scraped_tweets.csv')\n",
    "\n",
    "    ##Using sentiment analysis on df\n",
    "    sentiment_analysis(df)\n",
    "\n",
    "    ##Generate word clouds \n",
    "    make_WordCloud(df.Text)\n",
    "    make_WordCloud(df['Associated Hashtags'])\n",
    "\n",
    "    ##Generate pie chart\n",
    "    make_pieChart(df, term)\n",
    "\n",
    "    ##Generate RRL_table\n",
    "    make_RRL_table(df)\n",
    "\n",
    "#########################################################\n",
    "# Tweet out the results obtained from the data analysis #\n",
    "#########################################################\n",
    "\n",
    "def send_tweet_or_not():\n",
    "    yes = 'yes'\n",
    "    no = 'no'\n",
    "    \n",
    "    ##Showing the average value of the subjectivity & polarity analysis results obtained from the sentiment analysis with a \n",
    "    #brief description on how to interpret the results; attached as a description to the pie chart \n",
    "    def small_desc():\n",
    "        df = pd.read_csv('../TwitterBot/scraped_tweets.csv')\n",
    "        sentiment_analysis(df)\n",
    "\n",
    "        sub_mean = df.TextBlob_Subjectivity.mean()\n",
    "        pol_mean = df.TextBlob_Polarity.mean()\n",
    "\n",
    "        print('\\n')\n",
    "        sub_pol_mean = ('Additional information on the sentiment analysis: the average value for the subjectivity analysis is ' \n",
    "                       + str(sub_mean) \n",
    "                       + ' and for the polarity analysis it is ' \n",
    "                       + str(pol_mean) \n",
    "                       + '. Subjectivity is a float between [0,1]; where 1 means that the statement is more of a personal opinion '\n",
    "                         'and where 0 means that it is more objective or factual. Polarity is a float between [-1,1]; where a float '\n",
    "                         'closer to -1 means a negative statement, around 0 is considered neutral, and 1 is more positive.')\n",
    "        return(sub_pol_mean)\n",
    "\n",
    "    #prompting user to type close to close the web driver if a tweet was sent out\n",
    "    def close_webDriver():\n",
    "        close = 'close'\n",
    "\n",
    "        confirm_close = input('Please type \"close\" in order to close the web driver when you are finished: ')\n",
    "\n",
    "        if confirm_close.upper() == close.upper():\n",
    "            driver.close()\n",
    "            print('Web driver is now closing. Take care and see you again for another round of scraping!')\n",
    "        else:\n",
    "            print('Please type \"close\" in order to close the web driver.')\n",
    "            close_webDriver()\n",
    "    \n",
    "    tweet_or_not = input('The data are now stored away. If you want to tweet out your results, please enter \"yes\" to send a tweet or \"no\" to exit: ')\n",
    "    #when user wants to tweet out the results\n",
    "    if tweet_or_not.upper() == yes.upper():\n",
    "        additional_info = small_desc()\n",
    "\n",
    "        ##Link to compose a tweet; should not sign you out of the Twitter account\n",
    "        tweet_text = driver.get('http://www.twitter.com/compose/tweet')\n",
    "        sleep(3)\n",
    "\n",
    "        ##Sending out the following automated message\n",
    "        tweet_text = driver.find_element_by_xpath('//div[@aria-label=\"Tweet text\"][1]')\n",
    "        message = (\"A descriptive statistics on the latest 1000 tweets on \" + str(term) + \". 2 word clouds (tweets' text & associated \"\n",
    "                   \"hashtags; white & black background), a pie chart of a sentiment analysis done on the tweets' text and a table with \"\n",
    "                   \"total number of replies, retweets and likes.\")\n",
    "        tweet_text.send_keys(message)\n",
    "\n",
    "\n",
    "        ##Get directory path of all the images generated by the bot\n",
    "        images = ['wordcloud_text.png', 'wordcloud_hashtags.png', 'piechart_sentiment.png', 'RRL_table.png']\n",
    "        dir_path = [] #empty list to store the directory paths\n",
    "\n",
    "        for png_file in images:\n",
    "            dir_path.append(os.path.abspath(png_file))\n",
    "        dir_path\n",
    "\n",
    "        ##Upload the images generated by the bot\n",
    "        tweet_img = driver.find_element_by_xpath('//div[@dir=\"auto\"]')\n",
    "\n",
    "        for path in dir_path:\n",
    "            wait = WebDriverWait(tweet_img, 5)\n",
    "            input_xpath = '//input[@type=\"file\"]'\n",
    "            image_path = path\n",
    "            input_element = wait.until(EC.presence_of_element_located((By.XPATH, input_xpath)))\n",
    "            input_element.send_keys(image_path)\n",
    "\n",
    "        ##Add summary to the description of the pie chart of the sentiment analysis\n",
    "        #find the 'Add description' and click it\n",
    "        description = driver.find_element_by_xpath('//span[contains(text(),\"Add descriptions\")]').click()\n",
    "        sleep(1)\n",
    "        #moving to the pie chart image\n",
    "        description = driver.find_element_by_xpath('//div[@aria-label=\"Next image\"]//div[@dir=\"auto\"]').click()\n",
    "        sleep(1)\n",
    "        description = driver.find_element_by_xpath('//div[@aria-label=\"Next image\"]//div[@dir=\"auto\"]').click()\n",
    "        sleep(1)\n",
    "        #add summary to pie chart description as an additional information\n",
    "        description = driver.find_element_by_xpath('//textarea[@name=\"altTextInput\"]')\n",
    "        description.send_keys(additional_info)\n",
    "        sleep(2)\n",
    "        #find the 'Save' button and click it\n",
    "        description = driver.find_element_by_xpath('(//div[@role=\"button\"])[4]').click()\n",
    "        sleep(1)\n",
    "\n",
    "        ##Tweet out everything\n",
    "        tweet_out = driver.find_element_by_xpath('(//div[@role=\"button\"])[19]').click()\n",
    "        sleep(3)\n",
    "        print('A tweet about the latest 1000 tweets on ' + str(term) + ' has been made and sent out to be viewed by everyone using the '\n",
    "               'Twitter account @' + handle_or_phone + '. Thank you for using this Twitter Bot Scraper.')\n",
    "\n",
    "        ##Ask user to close the web driver if they are done\n",
    "        close_webDriver()\n",
    "    #when user does not want to tweet out results; web driver closes and exits program          \n",
    "    elif tweet_or_not.upper() == no.upper():\n",
    "        print('Thank you for using this Twitter Bot Scraper. Web driver is now closing. Take care and see you again for another round of scraping!')\n",
    "        driver.close()\n",
    "    else:\n",
    "        print('Not an available option. Please enter either \"yes\" or \"no\".')\n",
    "        send_tweet_or_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f70bc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "### TWITTER BOT SCRAPER PROGRAM ###\n",
    "###################################\n",
    "\n",
    "###################\n",
    "# Welcome message #\n",
    "###################\n",
    "\n",
    "print('Hello there and welcome to the Twitter Bot Scraper program! In order to ensure that the bot works smoothly, please make'\n",
    "      ' sure that the information provided are all correct. If you think that you made a mistake while providing certain '\n",
    "      'information, you can type \"hard reset\" at any time to restart everything at any point. Thank you and happy scraping!\\n')\n",
    "\n",
    "###############################################\n",
    "# Prompt for user's Twitter login information #\n",
    "###############################################\n",
    "\n",
    "#initialize variables that will be the id used to login into the Twitter account\n",
    "login_id = handle_or_phone = term = password = ''\n",
    "yes = 'yes'\n",
    "no = 'no'\n",
    "    \n",
    "#getting login id\n",
    "login_id = input('Please enter phone, email or username linked to your Twitter account:')\n",
    "if login_id == 'hard reset':\n",
    "    Twitter_login()\n",
    "#getting Twitter handle/phone in case Twitter thinks there is unusual activity\n",
    "handle_or_phone = input('Please enter your Twitter handle or phone: ') \n",
    "if handle_or_phone == 'hard reset':\n",
    "    Twitter_login()\n",
    "#getting the term that will be searched        \n",
    "term = input('Please enter the desired search term: ')\n",
    "if handle_or_phone == 'hard_reset':\n",
    "    Twitter_login()\n",
    "#getting password using getpass() to not openly reveal it\n",
    "print('Please enter your password: ')\n",
    "password = getpass.getpass()\n",
    "if password == 'hard_reset':\n",
    "    Twitter_login()\n",
    "    \n",
    "all_ids = [login_id, handle_or_phone, term, password]\n",
    "    \n",
    "#prompt user to confirm if the information provided are all correct \n",
    "def confirm():               \n",
    "    confirmation = input('Are you sure that all the information provided is correct? Please enter \"yes\" or \"no\": ')\n",
    "    if confirmation.upper() == yes.upper():\n",
    "        print('Twitter Bot is ready to scrape!')\n",
    "        login_id = all_ids[0]\n",
    "        handle_or_phone = all_ids[1]\n",
    "        term = all_ids[2]\n",
    "        password = all_ids[3]\n",
    "    elif confirmation.upper() == no.upper():\n",
    "        print('No worries, you can now start entering your information again.\\n ')\n",
    "        Twitter_login(login_id, handle_or_phone, term, password)\n",
    "    else:\n",
    "        print('Not an available option. Please enter \"yes\" or \"no\".')\n",
    "        confirm()\n",
    "    \n",
    "confirm()\n",
    "\n",
    "############################################################\n",
    "# Using user's information to login Twitter via Web driver #\n",
    "############################################################\n",
    "\n",
    "##Opening Chrome driver downloaded via https://chromedriver.chromium.org/downloads and selecting corresponding Chrome \n",
    " #version (in my case I downloaded the chromedriver_win32.zip for Chrome version 96). The exe file was then added to the \n",
    " #'final_project' --> 'TwitterBot' folder so it could be used\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window() #window is maximized in order to have access to Twitter's search query \n",
    "driver.get('http://www.twitter.com/login')\n",
    "sleep(3)\n",
    "\n",
    "##Login in with information provided above\n",
    "##For username\n",
    "username = driver.find_element_by_xpath('//input[@name=\"text\"]')\n",
    "username.send_keys(login_id)\n",
    "username.send_keys(Keys.RETURN)\n",
    "sleep(3)\n",
    "\n",
    "##For password & login in the Twitter account\n",
    "try:\n",
    "    username = driver.find_element_by_xpath('//input[@name=\"password\"]')\n",
    "#Sometimes Twitter might think there is some unusual activity and demands to input the account's username or phone\n",
    "except NoSuchElementException as error:\n",
    "    username = driver.find_element_by_xpath('//input[@name=\"text\"]')\n",
    "    username.send_keys(handle_or_phone)\n",
    "    username.send_keys(Keys.RETURN)\n",
    "    sleep(3)\n",
    "    #try to find input for password and try to login again\n",
    "    username = driver.find_element_by_xpath('//input[@name=\"password\"]')\n",
    "    username.send_keys(password)\n",
    "    username.send_keys(Keys.RETURN)\n",
    "    sleep(3)\n",
    "##If Twitter doesn't find any unusual activity, login normally\n",
    "else:\n",
    "    username.send_keys(password)\n",
    "    username.send_keys(Keys.RETURN)\n",
    "    sleep(3)\n",
    "\n",
    "##Finding the term in the search query of Twitter\n",
    "search_term = driver.get('http://www.twitter.com/search/')\n",
    "sleep(4)\n",
    "search_term = driver.find_element_by_xpath('(//input[@placeholder=\"Search Twitter\"])[1]')\n",
    "search_term.send_keys(term)\n",
    "search_term.send_keys(Keys.RETURN)\n",
    "sleep(2)\n",
    "\n",
    "##Sorting by latest\n",
    "driver.find_element_by_link_text('Latest').click()\n",
    "\n",
    "##User authorization to begin scraping process\n",
    "initiate_scraping(term)\n",
    "\n",
    "######################################################################\n",
    "### Scrolling Twitter website to scrape 1000 latest tweets on term ###\n",
    "######################################################################\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "##Get the desired number of tweets by scrolling the page to scrape new data\n",
    "data = [] #create an empty list to store scraped tweets\n",
    "unique_ids = set() #make a set of unique ids to make sure no same tweets are scraped\n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "scrolling = True\n",
    "max_data = 1000 #max number of data that will be scraped\n",
    "complete = '' #complete message for visual cue of the bot's scraping progress (will be updated as more data is being gathered)\n",
    "\n",
    "while scrolling and len(data) < max_data:\n",
    "    tweets = driver.find_elements_by_xpath('//article[@role=\"article\"]')\n",
    "    for t in tweets[-15:]:\n",
    "        tweet = scrape_tweet(t, term)\n",
    "        if tweet:\n",
    "            unique_id = ''.join(tweet[3]) #making a unique id by joining the text of each tweet\n",
    "            if unique_id not in unique_ids: #append unique id in set of ids if it is not in there yet\n",
    "                unique_ids.add(unique_id)\n",
    "                data.append(tweet)\n",
    "\n",
    "                #adding visual indication of how much data has been scraped at each 20% completion mark\n",
    "                if (len(data)/max_data) < 0.2 and complete != ('0% complete...'):\n",
    "                    complete = '0% complete...'\n",
    "                    print('Scraping Starts... Now!\\n' + complete)\n",
    "                elif (len(data)/max_data) >= 0.2 and (len(data)/max_data) < 0.4 and complete != ('20% complete...'):\n",
    "                    complete = '20% complete...'\n",
    "                    print(complete)\n",
    "                elif (len(data)/max_data) >= 0.4 and (len(data)/max_data) < 0.6 and complete != ('40% complete...'):\n",
    "                    complete = '40% complete...'\n",
    "                    print(complete)\n",
    "                elif (len(data)/max_data) >= 0.6 and (len(data)/max_data) < 0.8 and complete != ('60% complete...'):\n",
    "                    complete = '60% complete...'\n",
    "                    print(complete)\n",
    "                elif (len(data)/max_data) >= 0.8 and (len(data)/max_data) < 1.0 and complete != ('80% complete...'):\n",
    "                    complete = '80% complete...'\n",
    "                    print(complete)\n",
    "                elif (len(data)/max_data) >= 1.0:                    \n",
    "                    print('100% complete. Almost Done...')\n",
    "\n",
    "    #check scroll position; making sure it isnt stuck or if it ever reach the end        \n",
    "    scroll_attempt = 0\n",
    "    while True:       \n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1)\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position:\n",
    "            scroll_attempt += 1\n",
    "\n",
    "            #end of scroll region\n",
    "            if scroll_attempt >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else:\n",
    "                sleep(2) #attempt another scroll\n",
    "        else:\n",
    "            last_position = curr_position\n",
    "            break\n",
    "\n",
    "##Making sure that the data list is not higher than the number of scraped data desired\n",
    "while len(data) > max_data:\n",
    "    data.pop() #remove last data if length of data is higher than max_data\n",
    "\n",
    "##Visual display of scraper being done along with time taken to scrape the data\n",
    "print('Scraping is now complete.')\n",
    "print('\\nTime taken to scrape was (in seconds): ')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "###############################################################################\n",
    "# Display results along with information about the data that just got scraped #\n",
    "###############################################################################\n",
    "\n",
    "print('\\n')\n",
    "max_data_reached(data)\n",
    "print('\\n')\n",
    "make_csv(data)\n",
    "\n",
    "show_results()\n",
    "print('\\n')\n",
    "\n",
    "#################################\n",
    "# Ask user to send tweet or not #\n",
    "#################################\n",
    "\n",
    "send_tweet_or_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
